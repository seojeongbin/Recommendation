{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SR-GNN exercise \n",
    "- used data : kaggle competition (https://www.kaggle.com/retailrocket/ecommerce-dataset)\n",
    "- reference : https://colab.research.google.com/drive/1XwQ0njqSZL8vbHJMnRRHlH4ar0kYYFVz?usp=sharing#scrollTo=MKM4wAYG9ssq\n",
    "- medium : https://medium.com/stanford-cs224w/buy-this-session-based-recommendation-using-sr-gnn-d3415e393722\n",
    "- after-report : https://velog.io/@seojeongbin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python built-in libraries\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "# Import pip libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# Import torch packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "\n",
    "# Import PyG packages\n",
    "import torch_geometric as pyg\n",
    "import torch_geometric.data as pyg_data\n",
    "from torch_geometric.typing import Adj, OptTensor\n",
    "import torch_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       timestamp  visitorid event  itemid  transactionid\n",
      "0  1433221332117     257597  view  355908            NaN\n",
      "1  1433224214164     992329  view  248676            NaN\n",
      "2  1433221999827     111016  view  318965            NaN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2756101"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = r'C:\\Users\\SeoJeongBin\\Desktop\\Code\\Recommendation\\data\\SR-GNN\\events.csv\\events.csv'\n",
    "events_df = pd.read_csv(data_path)\n",
    "print(events_df.head(3))\n",
    "len(events_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['view' 'addtocart' 'transaction']\n",
      "[   nan  4000. 11117. ...  4385. 13872. 17579.]\n"
     ]
    }
   ],
   "source": [
    "print(events_df['event'].unique()) # 3types\n",
    "print(events_df['transactionid'].unique()) # unique type (only for transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "466867\n",
      "0\n",
      "1407579\n"
     ]
    }
   ],
   "source": [
    "print(min(events_df['itemid']))\n",
    "print(max(events_df['itemid']))\n",
    "print(min(events_df['visitorid']))\n",
    "print(max(events_df['visitorid']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Separating log data into sessions\n",
    "- event == view 인 경우만 처리\n",
    "- 방문 2회 이상 고객 대상 분석 (그래야 session 이란걸 정의할 수 있으니)\n",
    "- session을 두시간으로 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>visitorid</th>\n",
       "      <th>event</th>\n",
       "      <th>itemid</th>\n",
       "      <th>transactionid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1433221332117</td>\n",
       "      <td>257597</td>\n",
       "      <td>view</td>\n",
       "      <td>355908</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1433224214164</td>\n",
       "      <td>992329</td>\n",
       "      <td>view</td>\n",
       "      <td>248676</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1433221999827</td>\n",
       "      <td>111016</td>\n",
       "      <td>view</td>\n",
       "      <td>318965</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1433221955914</td>\n",
       "      <td>483717</td>\n",
       "      <td>view</td>\n",
       "      <td>253185</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1433221337106</td>\n",
       "      <td>951259</td>\n",
       "      <td>view</td>\n",
       "      <td>367447</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756096</th>\n",
       "      <td>1438398785939</td>\n",
       "      <td>591435</td>\n",
       "      <td>view</td>\n",
       "      <td>261427</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756097</th>\n",
       "      <td>1438399813142</td>\n",
       "      <td>762376</td>\n",
       "      <td>view</td>\n",
       "      <td>115946</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756098</th>\n",
       "      <td>1438397820527</td>\n",
       "      <td>1251746</td>\n",
       "      <td>view</td>\n",
       "      <td>78144</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756099</th>\n",
       "      <td>1438398530703</td>\n",
       "      <td>1184451</td>\n",
       "      <td>view</td>\n",
       "      <td>283392</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756100</th>\n",
       "      <td>1438400163914</td>\n",
       "      <td>199536</td>\n",
       "      <td>view</td>\n",
       "      <td>152913</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2664312 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  visitorid event  itemid  transactionid\n",
       "0        1433221332117     257597  view  355908            NaN\n",
       "1        1433224214164     992329  view  248676            NaN\n",
       "2        1433221999827     111016  view  318965            NaN\n",
       "3        1433221955914     483717  view  253185            NaN\n",
       "4        1433221337106     951259  view  367447            NaN\n",
       "...                ...        ...   ...     ...            ...\n",
       "2756096  1438398785939     591435  view  261427            NaN\n",
       "2756097  1438399813142     762376  view  115946            NaN\n",
       "2756098  1438397820527    1251746  view   78144            NaN\n",
       "2756099  1438398530703    1184451  view  283392            NaN\n",
       "2756100  1438400163914     199536  view  152913            NaN\n",
       "\n",
       "[2664312 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. filter only the 'view' events\n",
    "events_df_filtered = events_df.loc[events_df['event']=='view',:] # loc 내부 순서 실수 주의\n",
    "events_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp      visitorid  event  itemid  transactionid\n",
       "1438650539084  951289     view   5190    NaN              3\n",
       "1435308445514  1152713    view   427254  NaN              3\n",
       "1440044157440  1177355    view   161623  NaN              2\n",
       "1434468685577  618116     view   73917   NaN              2\n",
       "1439830202573  515669     view   56500   NaN              2\n",
       "                                                         ..\n",
       "1434504724447  1270836    view   53959   NaN              1\n",
       "1434504725643  562087     view   351415  NaN              1\n",
       "1434504726584  1326048    view   316575  NaN              1\n",
       "1434504727158  794304     view   293458  NaN              1\n",
       "1442545187788  1287495    view   98299   NaN              1\n",
       "Length: 2664218, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. filter out visitors with single clicks\n",
    "visit_counts_per_visitor = events_df_filtered.value_counts(dropna=False)\n",
    "visit_counts_per_visitor # 중복횟수를 우측에 카운트되게 나옴, 자동 내림차순(dropna=false는 nan값을 무시하지 않게 해줌)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1150086    6479\n",
       "530559     3623\n",
       "895999     2368\n",
       "152963     2304\n",
       "163561     2194\n",
       "           ... \n",
       "908147        1\n",
       "258979        1\n",
       "551045        1\n",
       "218233        1\n",
       "1184451       1\n",
       "Name: visitorid, Length: 1404179, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visit_counts_per_visitor = events_df_filtered['visitorid'].value_counts(dropna=False)\n",
    "visit_counts_per_visitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>visitorid</th>\n",
       "      <th>event</th>\n",
       "      <th>itemid</th>\n",
       "      <th>transactionid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1433221332117</td>\n",
       "      <td>257597</td>\n",
       "      <td>view</td>\n",
       "      <td>355908</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1433224214164</td>\n",
       "      <td>992329</td>\n",
       "      <td>view</td>\n",
       "      <td>248676</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1433221999827</td>\n",
       "      <td>111016</td>\n",
       "      <td>view</td>\n",
       "      <td>318965</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1433221955914</td>\n",
       "      <td>483717</td>\n",
       "      <td>view</td>\n",
       "      <td>253185</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1433224086234</td>\n",
       "      <td>972639</td>\n",
       "      <td>view</td>\n",
       "      <td>22556</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756092</th>\n",
       "      <td>1438398473572</td>\n",
       "      <td>709520</td>\n",
       "      <td>view</td>\n",
       "      <td>104512</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756094</th>\n",
       "      <td>1438399289446</td>\n",
       "      <td>701750</td>\n",
       "      <td>view</td>\n",
       "      <td>296172</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756095</th>\n",
       "      <td>1438400574346</td>\n",
       "      <td>289041</td>\n",
       "      <td>view</td>\n",
       "      <td>156947</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756098</th>\n",
       "      <td>1438397820527</td>\n",
       "      <td>1251746</td>\n",
       "      <td>view</td>\n",
       "      <td>78144</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756100</th>\n",
       "      <td>1438400163914</td>\n",
       "      <td>199536</td>\n",
       "      <td>view</td>\n",
       "      <td>152913</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1656582 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  visitorid event  itemid  transactionid\n",
       "0        1433221332117     257597  view  355908            NaN\n",
       "1        1433224214164     992329  view  248676            NaN\n",
       "2        1433221999827     111016  view  318965            NaN\n",
       "3        1433221955914     483717  view  253185            NaN\n",
       "5        1433224086234     972639  view   22556            NaN\n",
       "...                ...        ...   ...     ...            ...\n",
       "2756092  1438398473572     709520  view  104512            NaN\n",
       "2756094  1438399289446     701750  view  296172            NaN\n",
       "2756095  1438400574346     289041  view  156947            NaN\n",
       "2756098  1438397820527    1251746  view   78144            NaN\n",
       "2756100  1438400163914     199536  view  152913            NaN\n",
       "\n",
       "[1656582 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visitors_with_significant_visits = visit_counts_per_visitor[visit_counts_per_visitor > 1].index # 방문을 1번 넘게한 user\n",
    "events_df_filtered = events_df_filtered[events_df_filtered['visitorid'].isin(visitors_with_significant_visits)] # visitorid가 visitors_with_significant_visits 리스트에 속하는 경우만 반환\n",
    "events_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1656582it [02:22, 11596.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 396449 visitors left.\n",
      "소요시간 : 142.85715770721436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 'visits_by_visitors'\n",
    "# Let's group events and their timing data.\n",
    "import time # 이거 시간 꽤 걸림 (로컬기준 2분정도) => 셀 단위 특성 상 다시돌리는일많으니 이런거는 아예 저장을 시켜버려야함\n",
    "start = time.time()\n",
    "\n",
    "visits_by_visitors = {}\n",
    "for _, row in enumerate(tqdm(events_df_filtered.iterrows())): # tqdm : 진행률 보여줌 / iterrows : df행 단위로 받아올 수 있음. 튜플임 (행넘버, 내용 시리즈 객체)\n",
    "    timestamp, visitorid, event, itemid, transactionid = row[1].values\n",
    "\n",
    "    if visitorid not in visits_by_visitors:\n",
    "        visits_by_visitors[visitorid] = {'itemids': [], 'timestamps': []} # 전체 df돌면서 visitor마다 딕셔너리 키 생성해주고\n",
    "    visits_by_visitors[visitorid]['itemids'].append(itemid)\n",
    "    visits_by_visitors[visitorid]['timestamps'].append(timestamp)\n",
    "\n",
    "# print(visits_by_visitors)\n",
    "# 257597: {'itemids': [355908, 302696], 'timestamps': [1433221332117, 1433800821247]} 이런식으로 사용자 id별로 저장됨\n",
    "print(f'There are {len(visits_by_visitors)} visitors left.') # len(events_df_filtered['visitorid'].unique()) 걍 이거랑 똑같음\n",
    "print(f'소요시간 : {time.time() - start}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "len(sessions_by_visitors : 396449\n"
     ]
    }
   ],
   "source": [
    "# 'sessions_by_visitors'\n",
    "# We will separate sessions by 2 hours.\n",
    "delay = 2 * 3600 * 1000\n",
    "\n",
    "# 위에서 정의한 visits_by_visitors 딕셔너리를 통해 sessions_by_visitors 만들기\n",
    "sessions_by_visitors = {}\n",
    "for visitorid, visitor_dict in visits_by_visitors.items():\n",
    "    sessions = [[]]\n",
    "    events_sorted = sorted(zip(visitor_dict['timestamps'], visitor_dict['itemids']))\n",
    "    for i in range(len(events_sorted) - 1):\n",
    "        sessions[-1].append(events_sorted[i][1])\n",
    "        if (events_sorted[i+1][0] - events_sorted[i][0]) > delay:\n",
    "            sessions.append([])\n",
    "    sessions[-1].append(events_sorted[len(events_sorted) - 1][1])\n",
    "    sessions_by_visitors[visitorid] = sessions\n",
    "\n",
    "# print(sessions_by_visitors)\n",
    "# 이용자별로 session을 만드는과정임 고객 A : [[1시],[2시,2시10분,2시15분]] 이런느낌\n",
    "print()\n",
    "print(f'len(sessions_by_visitors : {len(sessions_by_visitors)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Splitting train and test dataset\n",
    "- 위에서 정의한 sessions_by_visitors 기준으로한 dataset 분할하기\n",
    "- now let's split data (train, valid, test) by user ids\n",
    "    - each user will only be found in one of the three splits\n",
    "    - We split like this because the model may 'cheat' from partial sessions of a user during training and use that information during test time.\n",
    "- 원래 추천시스템에서 데이터 분할하는게 일반적인 방법이랑 조금 다름\n",
    "    - 참고 : https://data-newbie.tistory.com/842"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, val, and test visitors: (317159, 39644, 39644)\n"
     ]
    }
   ],
   "source": [
    "# Adjsut sampling rate ([0, 1]) to generate smaller datasets.\n",
    "# Setting `sampling_rate` to 1 will lead to a full dataset split.\n",
    "sampling_rate = 1\n",
    "seed = 42 # random seed for reproducibility\n",
    "all_visitors = list(sessions_by_visitors.keys()) # 실수주의 : key()가 아니라 keys()임\n",
    "random.Random(seed).shuffle(all_visitors)\n",
    "\n",
    "num_train = int(len(all_visitors) * 0.8 * sampling_rate)\n",
    "num_val = int(len(all_visitors) * 0.1 * sampling_rate)\n",
    "num_test = int(len(all_visitors) * 0.1 * sampling_rate)\n",
    "\n",
    "train_visitors = all_visitors[:num_train]\n",
    "val_visitors = all_visitors[num_train : num_train+num_val]\n",
    "test_visitors = all_visitors[num_train+num_val:num_train+num_val+num_test]\n",
    "\n",
    "print(f'train, val, and test visitors: {len(train_visitors), len(val_visitors), len(test_visitors)}')\n",
    "# print(f'test_visitors : {test_visitors}') # visitors의 번호가 커다란 리스트에 하나로 다 들어가있는 형태임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에는 user만 하였으니 session도 train val test 나누어준다\n",
    "# each session => subsession (논문이 이런식으로 했음)\n",
    "def extract_subsessions(sessions):\n",
    "    \"\"\"Extracts all partial sessions from the sessions given.\n",
    "\n",
    "    For example, a session (1, 2, 3) should be augemnted to produce two\n",
    "    separate sessions (1, 2) and (1, 2, 3).\n",
    "    \"\"\"\n",
    "    all_sessions = []\n",
    "    for session in sessions:\n",
    "        for i in range(1, len(session)):\n",
    "            all_sessions.append(session[:i+1])\n",
    "    return all_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, val, and test sessions: (781928, 91317, 96896)\n"
     ]
    }
   ],
   "source": [
    "# Get sessions of each visitor, generate subsessions of each session, and put all the generated subsessions into right splits.\n",
    "# We generate subsessions according to the dataset generation policy suggested by the original SR-GNN paper.\n",
    "train_sessions, val_sessions, test_sessions = [], [], []\n",
    "for visitor in train_visitors:\n",
    "    train_sessions.extend(extract_subsessions(sessions_by_visitors[visitor]))\n",
    "for visitor in val_visitors:\n",
    "    val_sessions.extend(extract_subsessions(sessions_by_visitors[visitor]))\n",
    "for visitor in test_visitors:\n",
    "    test_sessions.extend(extract_subsessions(sessions_by_visitors[visitor]))\n",
    "    \n",
    "print(f'train, val, and test sessions: {len(train_sessions), len(val_sessions), len(test_sessions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.386579425363276"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sessions # 아이템이 클릭된 순서가 저장된 리스트들이 개별 sub session으로 리스트로 저장되어 있는 형태임\n",
    "# session 평균길이 구하기    \n",
    "length = len(test_sessions)\n",
    "total = 0\n",
    "for i in range(length):\n",
    "    total += len(test_sessions[i])\n",
    "avg = total / length\n",
    "avg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session을 train,val,test 별로 저장한다\n",
    "with open('raw/train.txt', 'wb') as f:\n",
    "    pickle.dump(train_sessions, f)\n",
    "with open('raw/val.txt', 'wb') as f:\n",
    "    pickle.dump(val_sessions, f)\n",
    "with open('raw/test.txt', 'wb') as f:\n",
    "    pickle.dump(test_sessions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Pipeline (tabular => graph)\n",
    "- 이거 잘되면 시각화도 하기!!\n",
    "- pyg InMemryDataset 방식 : https://www.youtube.com/watch?v=QLIkOtKS4os&list=PLV8yxwGOxvvoNkzPfCx2i8an--Tkt7O8Z&index=8\n",
    "    - 이 영상을 '마음으로' 이해해야 나중에 다른방식으로도 활용할 수 있을듯!!!!\n",
    "- pandas 방식 : https://www.youtube.com/watch?v=AQU3akndun4&list=PLV8yxwGOxvvoNkzPfCx2i8an--Tkt7O8Z&index=23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n>>> box =[1,1,0,2,1]\\n>>> senders, receivers = box[:-1], box[1:]\\n>>> import torch\\n>>> edge_index\\ntensor([[1, 1, 0, 2],\\n        [1, 0, 2, 1]])\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.factorzie\n",
    "'''\n",
    ">>> codes, uniques = pd.factorize(['b', 'b', 'a', 'c', 'b'])\n",
    ">>> codes\n",
    "array([0, 0, 1, 2, 0]...) # 먼저나온걸 0 으로 임베딩\n",
    ">>> uniques\n",
    "array(['b', 'a', 'c'], dtype=object)\n",
    "\n",
    ">>> codes, uniques = pd.factorize(['b', 'b', 'a', 'c', 'b'], sort=True)\n",
    ">>> codes\n",
    "array([1, 1, 0, 2, 1]...) # true하면 abc 순서대로 임베딩\n",
    ">>> uniques\n",
    "array(['a', 'b', 'c'], dtype=object)\n",
    "\n",
    "'''\n",
    "\n",
    "# edge_index\n",
    "'''\n",
    ">>> box =[1,1,0,2,1]\n",
    ">>> senders, receivers = box[:-1], box[1:]\n",
    ">>> import torch\n",
    ">>> edge_index\n",
    "tensor([[1, 1, 0, 2],\n",
    "        [1, 0, 2, 1]])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "이것들만 알아보면 됨!!\n",
    "1. pd.factorize 의 마음으로 이해\n",
    "2. pyg_data.Data(x=x, edge_index=edge_index, y=y) 구조\n",
    "3. self.collate\n",
    "'''\n",
    "# 이거 강의 다시 듣긴해야할듯!!\n",
    "\n",
    "class GraphDataset(pyg_data.InMemoryDataset):\n",
    "    def __init__(self, root, file_name, transform=None, pre_transform=None):\n",
    "        self.file_name = file_name\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0]) # 이게머지\n",
    "\n",
    "    @property # 이걸 입력으로 받아서 (참고로 그전에 raw/ 경로는 있어야 하는듯)\n",
    "    def raw_file_names(self): \n",
    "        return [f'{self.file_name}.txt']\n",
    "\n",
    "    @property # 이거로 바꿔서 출력하는 것 같음 (processed/ ~ 는 자동으로 만들어주는듯?)\n",
    "    def processed_file_names(self): # 밑에 process과정이 끝나면 processed/ 밑에 ~~.pt로 저장됨\n",
    "        return [f'{self.file_name}.pt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self): # raw_dir : 이것도 어디서 나오는지 궁금, files[0] 이유\n",
    "        raw_data_file = f'{self.raw_dir}/{self.raw_file_names[0]}'\n",
    "        with open(raw_data_file, 'rb') as f:\n",
    "            sessions = pickle.load(f) # raw 파일 자체가 session이 저장된 sessions임!\n",
    "        data_list = [] # 이게 결국 sub graph 모음인듯?\n",
    "\n",
    "        for session in sessions:\n",
    "            session, y = session[:-1], session[-1] # y : target label of each session (세션에서 가장 마지막 클릭한 아이템)\n",
    "            codes, uniques = pd.factorize(session)\n",
    "            senders, receivers = codes[:-1], codes[1:] # 맨뒤에꺼빼고 sender의 기능을 할 수 있지, 맨앞에꺼빼고 receiver기능할 수 있다\n",
    "\n",
    "            # Build Data instance\n",
    "            edge_index = torch.tensor([senders, receivers], dtype=torch.long) # 이웃관계 나타내는 2개열을 갖는 2차원 텐서 : 송신자, 수신자 (여기서는 방향있는 그래프임)\n",
    "            x = torch.tensor(uniques, dtype=torch.long).unsqueeze(1)\n",
    "            # x is the list of original unique item ids.\n",
    "            # x => node가 갖는 값을 벡터로 넣는거임 (결국 매트릭스, pyg.ipynb 표현방식1 참고)\n",
    "            y = torch.tensor([y], dtype=torch.long) # 각 세션에서 마지막 아이템\n",
    "            data_list.append(pyg_data.Data(x=x, edge_index=edge_index, y=y)) # 별거없는듯...?!\n",
    "            \n",
    "\n",
    "        data, slices = self.collate(data_list) # self.collate : 거대한 리스트를 그냥 다루면 굉장히 느림 & batch로 다룰 수 있게 됨\n",
    "        torch.save((data, slices), self.processed_paths[0]) # 여기도 0이 뭔 의미일까"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 전 graph 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 118652/781928 [00:20<01:53, 5869.42it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\SeoJeongBin\\Desktop\\Code\\Recommendation\\SR-GNN\\SR-GNN.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/SeoJeongBin/Desktop/Code/Recommendation/SR-GNN/SR-GNN.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m data_y \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/SeoJeongBin/Desktop/Code/Recommendation/SR-GNN/SR-GNN.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(train_len)):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/SeoJeongBin/Desktop/Code/Recommendation/SR-GNN/SR-GNN.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     data_y\u001b[39m.\u001b[39mappend(train_dataset[i]\u001b[39m.\u001b[39my)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/SeoJeongBin/Desktop/Code/Recommendation/SR-GNN/SR-GNN.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m data_y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(data_y,\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SeoJeongBin/Desktop/Code/Recommendation/SR-GNN/SR-GNN.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(data_y))\n",
      "File \u001b[1;32mc:\\Users\\SeoJeongBin\\miniconda3\\envs\\kotorch\\lib\\site-packages\\torch_geometric\\data\\dataset.py:197\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"In case :obj:`idx` is of type integer, will return the data object\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[39mat index :obj:`idx` (and transforms it in case :obj:`transform` is\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[39mpresent).\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[39mIn case :obj:`idx` is a slicing object, *e.g.*, :obj:`[2:5]`, a list, a\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[39mtuple, or a :obj:`torch.Tensor` or :obj:`np.ndarray` of type long or\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[39mbool, will return a subset of the dataset at the specified indices.\"\"\"\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(idx, (\u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39minteger))\n\u001b[0;32m    194\u001b[0m         \u001b[39mor\u001b[39;00m (\u001b[39misinstance\u001b[39m(idx, Tensor) \u001b[39mand\u001b[39;00m idx\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m    195\u001b[0m         \u001b[39mor\u001b[39;00m (\u001b[39misinstance\u001b[39m(idx, np\u001b[39m.\u001b[39mndarray) \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39misscalar(idx))):\n\u001b[1;32m--> 197\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices()[idx])\n\u001b[0;32m    198\u001b[0m     data \u001b[39m=\u001b[39m data \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(data)\n\u001b[0;32m    199\u001b[0m     \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\SeoJeongBin\\miniconda3\\envs\\kotorch\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:90\u001b[0m, in \u001b[0;36mInMemoryDataset.get\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_list[idx] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     88\u001b[0m     \u001b[39mreturn\u001b[39;00m copy\u001b[39m.\u001b[39mcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_list[idx])\n\u001b[1;32m---> 90\u001b[0m data \u001b[39m=\u001b[39m separate(\n\u001b[0;32m     91\u001b[0m     \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m,\n\u001b[0;32m     92\u001b[0m     batch\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata,\n\u001b[0;32m     93\u001b[0m     idx\u001b[39m=\u001b[39;49midx,\n\u001b[0;32m     94\u001b[0m     slice_dict\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mslices,\n\u001b[0;32m     95\u001b[0m     decrement\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     96\u001b[0m )\n\u001b[0;32m     98\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_list[idx] \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mcopy(data)\n\u001b[0;32m    100\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\SeoJeongBin\\miniconda3\\envs\\kotorch\\lib\\site-packages\\torch_geometric\\data\\separate.py:37\u001b[0m, in \u001b[0;36mseparate\u001b[1;34m(cls, batch, idx, slice_dict, inc_dict, decrement)\u001b[0m\n\u001b[0;32m     35\u001b[0m         slices \u001b[39m=\u001b[39m slice_dict[attr]\n\u001b[0;32m     36\u001b[0m         incs \u001b[39m=\u001b[39m inc_dict[attr] \u001b[39mif\u001b[39;00m decrement \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     data_store[attr] \u001b[39m=\u001b[39m _separate(attr, batch_store[attr], idx, slices,\n\u001b[0;32m     38\u001b[0m                                  incs, batch, batch_store, decrement)\n\u001b[0;32m     40\u001b[0m \u001b[39m# The `num_nodes` attribute needs special treatment, as we cannot infer\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39m# the real number of nodes from the total number of nodes alone:\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(batch_store, \u001b[39m'\u001b[39m\u001b[39m_num_nodes\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\SeoJeongBin\\miniconda3\\envs\\kotorch\\lib\\site-packages\\torch_geometric\\data\\separate.py:63\u001b[0m, in \u001b[0;36m_separate\u001b[1;34m(key, value, idx, slices, incs, batch, store, decrement)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Tensor):\n\u001b[0;32m     60\u001b[0m     \u001b[39m# Narrow a `torch.Tensor` based on `slices`.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[39m# NOTE: We need to take care of decrementing elements appropriately.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     cat_dim \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39m__cat_dim__(key, value, store)\n\u001b[1;32m---> 63\u001b[0m     start, end \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(slices[idx]), \u001b[39mint\u001b[39m(slices[idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m])\n\u001b[0;32m     64\u001b[0m     value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mnarrow(cat_dim \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m, start, end \u001b[39m-\u001b[39m start)\n\u001b[0;32m     65\u001b[0m     value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m) \u001b[39mif\u001b[39;00m cat_dim \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m value\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_dataset = GraphDataset('./', 'train')\n",
    "# train_len = len(train_dataset)\n",
    "# print(train_len)\n",
    "\n",
    "# data_y = []\n",
    "# for i in tqdm(range(train_len)):\n",
    "#     data_y.append(train_dataset[i].y)\n",
    "# data_y = torch.cat(data_y,0)\n",
    "\n",
    "# print(len(data_y))\n",
    "# data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'visualize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\SeoJeongBin\\Desktop\\Code\\Recommendation\\SR-GNN\\SR-GNN.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/SeoJeongBin/Desktop/Code/Recommendation/SR-GNN/SR-GNN.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m GraphDataset(\u001b[39m'\u001b[39m\u001b[39m./\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SeoJeongBin/Desktop/Code/Recommendation/SR-GNN/SR-GNN.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# train_dataset = to_networkx(train_dataset, to_undirected=True)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/SeoJeongBin/Desktop/Code/Recommendation/SR-GNN/SR-GNN.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m visualize(train_dataset, color\u001b[39m=\u001b[39mdata_y[:\u001b[39m34\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'visualize' is not defined"
     ]
    }
   ],
   "source": [
    "# # 다른 폴더에 있는 시각화 모듈 불러오기 (sys.path.append를 쓰면서 절대경로로 공통상위까지 잡는게 포인트임!)\n",
    "# import sys\n",
    "# sys.path.append(r'C:\\Users\\SeoJeongBin\\Desktop\\Code\\Recommendation')\n",
    "# import NetworkX.visualize\n",
    "\n",
    "# # 이거같은경우에는 subgraph 별 item node 화살표임.\n",
    "# # 일단 여기 visualize 함수는 노드 classification인 경우란걸 알기 (그리고 학습했으면 epoch, loss도 받을 수 있다!)\n",
    "# from torch_geometric.utils import to_networkx\n",
    "# train_dataset = GraphDataset('./', 'train')\n",
    "\n",
    "# # train_dataset = to_networkx(train_dataset, to_undirected=True)\n",
    "# visualize(train_dataset, color=data_y[:34]) # 왠지 모르겠으나 34만됨..........\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx 이용해서 그림그리기 https://velog.io/@djm0727/ML-with-Graph-Colab0#visualization\n",
    "# 이종노드라는 점, 문제의 목적에 따라 시각화 방법을 달리해야할듯\n",
    "# import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design\n",
    "our gated session graph layer has two main parts\n",
    "- 1. message propagation to create an adjacency matrix(self.propage)\n",
    "- 2. the grou cell(self.gru)\n",
    "    - we will put these inside the forward() function\n",
    "    - We only use one layer for our GatedSessionGraphConv implementation for simplicity. Also, our sessions have average length < 5, so we do not need a large receptive field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n이해하기 상당히 어려운것들이 몇가지 있음\\n- channel 의미\\n- gated~가 무엇인지\\n- propagate가 무엇인지\\n- aggregate가 무엇인지\\n일단 검색결과 gnn 중에서도 pyg에 해당하는 사항, 이거는 rnn 관련이란걸 알게됨. gatedgnn이 gnn += rnn인듯. gru는 lstm 이후 나온 장기기억 방법..음..\\n'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메모\n",
    "'''\n",
    "이해하기 상당히 어려운것들이 몇가지 있음\n",
    "- channel 의미\n",
    "- gated~가 무엇인지\n",
    "- propagate가 무엇인지\n",
    "- aggregate가 무엇인지\n",
    "일단 검색결과 gnn 중에서도 pyg에 해당하는 사항, 이거는 rnn 관련이란걸 알게됨. gatedgnn이 gnn += rnn인듯. gru는 lstm 이후 나온 장기기억 방법..음..\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedSessionGraphConv(pyg.nn.MessagePassing):\n",
    "    def __init__(self, out_channels, aggr : str = 'add', **kwargs):\n",
    "        super().__init__(aggr=aggr, **kwargs)\n",
    "        \n",
    "        self.out_channels = out_channels # channel이 뭘까..\n",
    "        self.gru = torch.nn.GRUCell(out_channels, out_channels, bias=False)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        m = self.propagate(edge_index, x=x, size=None)\n",
    "        x = self.gru(m,x)\n",
    "        return x\n",
    "    \n",
    "    def message(self, x_j): # 이거 네이밍에 주의해야한다는 것 같았음\n",
    "        return x_j\n",
    "    \n",
    "    def message_and_aggregate(self, adj_t, x):\n",
    "        return matmul(adj_t, x, reduce=self.aggr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRGNN(nn.Module):\n",
    "    def __init__(self, hidden_size, n_items):\n",
    "        super(SRGNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_items = n_items\n",
    "\n",
    "        self.embedding = nn.Embedding(self.n_items, self.hidden_size)\n",
    "        self.gated = GatedSessionGraphConv(self.hidden_size) # 저위에서 선언하고 이걸 객체로 받은느낌인데.. 다시한번 passing 대해서 두개 글 읽어보기\n",
    "\n",
    "        self.q = nn.Linear(self.hidden_size, 1)\n",
    "        self.W_1 = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.W_2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.W_3 = nn.Linear(2 * self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch_map = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # (0)\n",
    "        embedding = self.embedding(x).squeeze()\n",
    "\n",
    "        # (1)-(5)\n",
    "        v_i = self.gated(embedding, edge_index)\n",
    "\n",
    "        # Divide nodes by session\n",
    "        # For the detailed explanation of what is happening below, please refer\n",
    "        # to the Medium blog post.\n",
    "        sections = list(torch.bincount(batch_map).cpu())\n",
    "        v_i_split = torch.split(v_i, sections)\n",
    "\n",
    "        v_n, v_n_repeat = [], []\n",
    "        for session in v_i_split:\n",
    "            v_n.append(session[-1])\n",
    "            v_n_repeat.append(\n",
    "                session[-1].view(1, -1).repeat(session.shape[0], 1))\n",
    "        v_n, v_n_repeat = torch.stack(v_n), torch.cat(v_n_repeat, dim=0)\n",
    "\n",
    "        q1 = self.W_1(v_n_repeat)\n",
    "        q2 = self.W_2(v_i)\n",
    "\n",
    "        # (6)\n",
    "        alpha = self.q(F.sigmoid(q1 + q2))\n",
    "        s_g_split = torch.split(alpha * v_i, sections)\n",
    "\n",
    "        s_g = []\n",
    "        for session in s_g_split:\n",
    "            s_g_session = torch.sum(session, dim=0)\n",
    "            s_g.append(s_g_session)\n",
    "        s_g = torch.stack(s_g)\n",
    "\n",
    "        # (7)\n",
    "        s_l = v_n\n",
    "        s_h = self.W_3(torch.cat([s_l, s_g], dim=-1))\n",
    "\n",
    "        # (8)\n",
    "        z = torch.mm(self.embedding.weight, s_h.T).T\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters.\n",
    "# Code taken from 2021 Fall CS224W Colab assignments.\n",
    "args = {\n",
    "    'batch_size': 100,\n",
    "    'hidden_dim': 32,\n",
    "    'epochs': 10, # 100\n",
    "    'l2_penalty': 0.00001,\n",
    "    'weight_decay': 0.1,\n",
    "    'step': 30,\n",
    "    'lr': 0.001,\n",
    "    'num_items': 466868}\n",
    "\n",
    "class objectview(object): # 이게뭐여\n",
    "    def __init__(self, d): \n",
    "        self.__dict__ = d\n",
    "\n",
    "args = objectview(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "# device = 'cuda' 코랩 gpu로 돌린다면 꼭 이거로 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    # Prepare data pipeline\n",
    "    train_dataset = GraphDataset('./', 'train')\n",
    "    train_loader = pyg_data.DataLoader(train_dataset,\n",
    "                                       batch_size=args.batch_size,\n",
    "                                       shuffle=False,\n",
    "                                       drop_last=True)\n",
    "    val_dataset = GraphDataset('./', 'val')\n",
    "    val_loader = pyg_data.DataLoader(val_dataset,\n",
    "                                     batch_size=args.batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     drop_last=True)\n",
    "\n",
    "    # Build model\n",
    "    model = SRGNN(args.hidden_dim, args.num_items).to(device)\n",
    "\n",
    "    # Get training components\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=args.lr,\n",
    "                                 weight_decay=args.l2_penalty)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                          step_size=args.step,\n",
    "                                          gamma=args.weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train\n",
    "    losses = []\n",
    "    test_accs = []\n",
    "    top_k_accs = []\n",
    "\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for _, batch in enumerate(tqdm(train_loader)):\n",
    "            batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred = model(batch)\n",
    "            label = batch.y\n",
    "            loss = criterion(pred, label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "        total_loss /= len(train_loader.dataset)\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "          test_acc, top_k_acc = test(val_loader, model, is_validation=True)\n",
    "          print(test_acc)\n",
    "          test_accs.append(test_acc)\n",
    "          top_k_accs.append(top_k_acc)\n",
    "          if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "          test_accs.append(test_accs[-1])\n",
    "  \n",
    "    return test_accs, top_k_accs, losses, best_model, best_acc, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, test_model, is_validation=False, save_model_preds=False):\n",
    "    test_model.eval()\n",
    "\n",
    "    # Define K for Hit@K metrics.\n",
    "    k = 20\n",
    "    correct = 0\n",
    "    top_k_correct = 0\n",
    "\n",
    "    for _, data in enumerate(tqdm(loader)):\n",
    "        data.to(device)\n",
    "        with torch.no_grad():\n",
    "            # max(dim=1) returns values, indices tuple; only need indices\n",
    "            score = test_model(data)\n",
    "            pred = score.max(dim=1)[1]\n",
    "            label = data.y\n",
    "\n",
    "        if save_model_preds:\n",
    "          data = {}\n",
    "          data['pred'] = pred.view(-1).cpu().detach().numpy()\n",
    "          data['label'] = label.view(-1).cpu().detach().numpy()\n",
    "\n",
    "          df = pd.DataFrame(data=data)\n",
    "          # Save locally as csv\n",
    "          df.to_csv('pred.csv', sep=',', index=False)\n",
    "            \n",
    "        correct += pred.eq(label).sum().item()\n",
    "\n",
    "        # We calculate Hit@K accuracy only at test time.\n",
    "        if not is_validation:\n",
    "            score = score.cpu().detach().numpy()\n",
    "            for row in range(pred.size(0)):\n",
    "                top_k_pred = np.argpartition(score[row], -k)[-k:]\n",
    "                if label[row].item() in top_k_pred:\n",
    "                    top_k_correct += 1\n",
    "    \n",
    "    if not is_validation:\n",
    "        return correct / len(loader), top_k_correct / len(loader)\n",
    "    else:\n",
    "        return correct / len(loader), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 1711/7819 [48:59<2:54:55,  1.72s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\SeoJeongBin\\Desktop\\Code\\Recommendation\\SR-GNN\\SR-GNN.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/SeoJeongBin/Desktop/Code/Recommendation/SR-GNN/SR-GNN.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_accs, top_k_accs, losses, best_model, best_acc, test_loader \u001b[39m=\u001b[39m train(args) \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/SeoJeongBin/Desktop/Code/Recommendation/SR-GNN/SR-GNN.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(test_accs, top_k_accs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/SeoJeongBin/Desktop/Code/Recommendation/SR-GNN/SR-GNN.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMaximum test set accuracy: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mmax\u001b[39m(test_accs)))\n",
      "\u001b[1;32mc:\\Users\\SeoJeongBin\\Desktop\\Code\\Recommendation\\SR-GNN\\SR-GNN.ipynb Cell 33\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SeoJeongBin/Desktop/Code/Recommendation/SR-GNN/SR-GNN.ipynb#X41sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SeoJeongBin/Desktop/Code/Recommendation/SR-GNN/SR-GNN.ipynb#X41sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/SeoJeongBin/Desktop/Code/Recommendation/SR-GNN/SR-GNN.ipynb#X41sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mfor\u001b[39;00m _, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(train_loader)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SeoJeongBin/Desktop/Code/Recommendation/SR-GNN/SR-GNN.ipynb#X41sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     batch\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SeoJeongBin/Desktop/Code/Recommendation/SR-GNN/SR-GNN.ipynb#X41sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\SeoJeongBin\\miniconda3\\envs\\kotorch\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SeoJeongBin\\miniconda3\\envs\\kotorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\SeoJeongBin\\miniconda3\\envs\\kotorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mc:\\Users\\SeoJeongBin\\miniconda3\\envs\\kotorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\SeoJeongBin\\miniconda3\\envs\\kotorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\SeoJeongBin\\miniconda3\\envs\\kotorch\\lib\\site-packages\\torch_geometric\\data\\dataset.py:197\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"In case :obj:`idx` is of type integer, will return the data object\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[39mat index :obj:`idx` (and transforms it in case :obj:`transform` is\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[39mpresent).\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[39mIn case :obj:`idx` is a slicing object, *e.g.*, :obj:`[2:5]`, a list, a\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[39mtuple, or a :obj:`torch.Tensor` or :obj:`np.ndarray` of type long or\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[39mbool, will return a subset of the dataset at the specified indices.\"\"\"\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(idx, (\u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39minteger))\n\u001b[0;32m    194\u001b[0m         \u001b[39mor\u001b[39;00m (\u001b[39misinstance\u001b[39m(idx, Tensor) \u001b[39mand\u001b[39;00m idx\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m    195\u001b[0m         \u001b[39mor\u001b[39;00m (\u001b[39misinstance\u001b[39m(idx, np\u001b[39m.\u001b[39mndarray) \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39misscalar(idx))):\n\u001b[1;32m--> 197\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices()[idx])\n\u001b[0;32m    198\u001b[0m     data \u001b[39m=\u001b[39m data \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(data)\n\u001b[0;32m    199\u001b[0m     \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\SeoJeongBin\\miniconda3\\envs\\kotorch\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:90\u001b[0m, in \u001b[0;36mInMemoryDataset.get\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_list[idx] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     88\u001b[0m     \u001b[39mreturn\u001b[39;00m copy\u001b[39m.\u001b[39mcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_list[idx])\n\u001b[1;32m---> 90\u001b[0m data \u001b[39m=\u001b[39m separate(\n\u001b[0;32m     91\u001b[0m     \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m,\n\u001b[0;32m     92\u001b[0m     batch\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata,\n\u001b[0;32m     93\u001b[0m     idx\u001b[39m=\u001b[39;49midx,\n\u001b[0;32m     94\u001b[0m     slice_dict\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mslices,\n\u001b[0;32m     95\u001b[0m     decrement\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     96\u001b[0m )\n\u001b[0;32m     98\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_list[idx] \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mcopy(data)\n\u001b[0;32m    100\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\SeoJeongBin\\miniconda3\\envs\\kotorch\\lib\\site-packages\\torch_geometric\\data\\separate.py:37\u001b[0m, in \u001b[0;36mseparate\u001b[1;34m(cls, batch, idx, slice_dict, inc_dict, decrement)\u001b[0m\n\u001b[0;32m     35\u001b[0m         slices \u001b[39m=\u001b[39m slice_dict[attr]\n\u001b[0;32m     36\u001b[0m         incs \u001b[39m=\u001b[39m inc_dict[attr] \u001b[39mif\u001b[39;00m decrement \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     data_store[attr] \u001b[39m=\u001b[39m _separate(attr, batch_store[attr], idx, slices,\n\u001b[0;32m     38\u001b[0m                                  incs, batch, batch_store, decrement)\n\u001b[0;32m     40\u001b[0m \u001b[39m# The `num_nodes` attribute needs special treatment, as we cannot infer\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39m# the real number of nodes from the total number of nodes alone:\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(batch_store, \u001b[39m'\u001b[39m\u001b[39m_num_nodes\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\SeoJeongBin\\miniconda3\\envs\\kotorch\\lib\\site-packages\\torch_geometric\\data\\separate.py:64\u001b[0m, in \u001b[0;36m_separate\u001b[1;34m(key, value, idx, slices, incs, batch, store, decrement)\u001b[0m\n\u001b[0;32m     62\u001b[0m cat_dim \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39m__cat_dim__(key, value, store)\n\u001b[0;32m     63\u001b[0m start, end \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(slices[idx]), \u001b[39mint\u001b[39m(slices[idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m])\n\u001b[1;32m---> 64\u001b[0m value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39;49mnarrow(cat_dim \u001b[39mor\u001b[39;49;00m \u001b[39m0\u001b[39;49m, start, end \u001b[39m-\u001b[39;49m start)\n\u001b[0;32m     65\u001b[0m value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m) \u001b[39mif\u001b[39;00m cat_dim \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m value\n\u001b[0;32m     66\u001b[0m \u001b[39mif\u001b[39;00m decrement \u001b[39mand\u001b[39;00m (incs\u001b[39m.\u001b[39mdim() \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mint\u001b[39m(incs[idx]) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# epochs 10으로 해도 하루종일 걸림. 진짜 학습이 목적이라면 무조건 코랩 gpu로 돌려야함\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "test_accs, top_k_accs, losses, best_model, best_acc, test_loader = train(args) \n",
    "\n",
    "print(test_accs, top_k_accs)\n",
    "print(\"Maximum test set accuracy: {0}\".format(max(test_accs)))\n",
    "print(\"Minimum loss: {0}\".format(min(losses)))\n",
    "\n",
    "# plt.title(dataset.name)\n",
    "plt.plot(losses, label=\"training loss\" + \" - \")\n",
    "plt.plot(test_accs, label=\"test accuracy\" + \" - \")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\n *소요시간 : {time.time() - start}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('kotorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e394b0e5950b2ed9ac02d3d0f43da50df004fa03f183c9ffe2862d4a06a95d21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
